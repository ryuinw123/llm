{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5566a2e2-bced-4f41-9b3d-a4c01e662613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio==3.44.4 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (3.44.4)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (3.1.2)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (5.1.2)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (4.8.0)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (6.0.1)\n",
      "Requirement already satisfied: numpy~=1.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (1.26.1)\n",
      "Requirement already satisfied: matplotlib~=3.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (3.8.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (0.24.0.post1)\n",
      "Requirement already satisfied: httpx in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (0.0.6)\n",
      "Requirement already satisfied: gradio-client==0.5.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (0.5.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (2.1.3)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (6.1.1)\n",
      "Requirement already satisfied: markupsafe~=2.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (3.9.10)\n",
      "Requirement already satisfied: packaging in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (23.2)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (23.2.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (2.4.2)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (10.1.0)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (0.3.1)\n",
      "Requirement already satisfied: fastapi in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (0.104.1)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (2.10.0)\n",
      "Requirement already satisfied: pydub in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (0.25.1)\n",
      "Requirement already satisfied: requests~=2.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (2.31.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.14.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (0.17.3)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (11.0.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio-client==0.5.1->gradio==3.44.4) (2023.10.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio==3.44.4) (4.19.2)\n",
      "Requirement already satisfied: toolz in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio==3.44.4) (0.12.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from huggingface-hub>=0.14.0->gradio==3.44.4) (4.66.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from huggingface-hub>=0.14.0->gradio==3.44.4) (3.13.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from importlib-resources<7.0,>=1.3->gradio==3.44.4) (3.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from matplotlib~=3.0->gradio==3.44.4) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from matplotlib~=3.0->gradio==3.44.4) (3.1.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from matplotlib~=3.0->gradio==3.44.4) (1.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from matplotlib~=3.0->gradio==3.44.4) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from matplotlib~=3.0->gradio==3.44.4) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from matplotlib~=3.0->gradio==3.44.4) (4.44.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from pandas<3.0,>=1.0->gradio==3.44.4) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from pandas<3.0,>=1.0->gradio==3.44.4) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.44.4) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.44.4) (2.10.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests~=2.0->gradio==3.44.4) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests~=2.0->gradio==3.44.4) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests~=2.0->gradio==3.44.4) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests~=2.0->gradio==3.44.4) (2023.7.22)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from uvicorn>=0.14.0->gradio==3.44.4) (0.14.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from uvicorn>=0.14.0->gradio==3.44.4) (8.1.7)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from fastapi->gradio==3.44.4) (0.27.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from fastapi->gradio==3.44.4) (3.7.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from httpx->gradio==3.44.4) (1.3.0)\n",
      "Requirement already satisfied: httpcore in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from httpx->gradio==3.44.4) (1.0.2)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio==3.44.4) (1.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from click>=7.0->uvicorn>=0.14.0->gradio==3.44.4) (0.4.6)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.44.4) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.44.4) (0.12.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.44.4) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.44.4) (2023.7.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==3.44.4) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\ryu\\Desktop\\llm\\openthaigpt-preview\\my-env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[sentencepiece] in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (4.35.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers[sentencepiece]) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers[sentencepiece]) (2.31.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers[sentencepiece]) (23.2)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers[sentencepiece]) (0.14.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers[sentencepiece]) (2023.10.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers[sentencepiece]) (3.13.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers[sentencepiece]) (0.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers[sentencepiece]) (0.17.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers[sentencepiece]) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers[sentencepiece]) (1.26.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers[sentencepiece]) (4.25.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers[sentencepiece]) (0.1.99)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers[sentencepiece]) (4.8.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers[sentencepiece]) (2023.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from tqdm>=4.27->transformers[sentencepiece]) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests->transformers[sentencepiece]) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests->transformers[sentencepiece]) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests->transformers[sentencepiece]) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests->transformers[sentencepiece]) (2.0.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\ryu\\Desktop\\llm\\openthaigpt-preview\\my-env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/peft.git\n",
      "  Cloning https://github.com/huggingface/peft.git to c:\\users\\ryu\\appdata\\local\\temp\\pip-req-build-75uahq3o\n",
      "  Resolved https://github.com/huggingface/peft.git to commit 5d84484079ee72c92678eadb273d3fe0241ed5ea\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from peft==0.6.2.dev0) (2.1.0+cu118)\n",
      "Requirement already satisfied: psutil in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from peft==0.6.2.dev0) (5.9.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from peft==0.6.2.dev0) (1.26.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from peft==0.6.2.dev0) (0.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from peft==0.6.2.dev0) (23.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from peft==0.6.2.dev0) (4.66.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from peft==0.6.2.dev0) (0.24.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from peft==0.6.2.dev0) (6.0.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from peft==0.6.2.dev0) (4.35.0)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from accelerate>=0.21.0->peft==0.6.2.dev0) (0.17.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torch>=1.13.0->peft==0.6.2.dev0) (2023.10.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torch>=1.13.0->peft==0.6.2.dev0) (3.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torch>=1.13.0->peft==0.6.2.dev0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torch>=1.13.0->peft==0.6.2.dev0) (4.8.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torch>=1.13.0->peft==0.6.2.dev0) (3.2.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torch>=1.13.0->peft==0.6.2.dev0) (1.12)\n",
      "Requirement already satisfied: colorama in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from tqdm->peft==0.6.2.dev0) (0.4.6)\n",
      "Requirement already satisfied: requests in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers->peft==0.6.2.dev0) (2.31.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers->peft==0.6.2.dev0) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers->peft==0.6.2.dev0) (0.14.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from jinja2->torch>=1.13.0->peft==0.6.2.dev0) (2.1.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests->transformers->peft==0.6.2.dev0) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests->transformers->peft==0.6.2.dev0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests->transformers->peft==0.6.2.dev0) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests->transformers->peft==0.6.2.dev0) (3.3.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from sympy->torch>=1.13.0->peft==0.6.2.dev0) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git 'C:\\Users\\ryu\\AppData\\Local\\Temp\\pip-req-build-75uahq3o'\n",
      "WARNING: You are using pip version 22.0.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\ryu\\Desktop\\llm\\openthaigpt-preview\\my-env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio==3.44.4\n",
    "!pip install transformers[sentencepiece]\n",
    "!pip install git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3e9ba52-6f26-40d6-9b0e-1c5138263251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (0.16.0+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: filelock in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torchvision) (1.26.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests->torchvision) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\ryu\\Desktop\\llm\\openthaigpt-preview\\my-env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f8d0ba9-a44e-4909-ba4e-63984cf07964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryu\\Desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import gradio as gr\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
    "import json\n",
    "import os.path as osp\n",
    "from typing import Union\n",
    "import gc\n",
    "import traceback\n",
    "from queue import Queue\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9032369-2421-471f-8d72-eac723bf68fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu_memory():\n",
    "    # Delete model and tensors if they are defined\n",
    "    global model, inputs, input_ids\n",
    "    if 'model' in globals():\n",
    "        del model\n",
    "    if 'inputs' in globals():\n",
    "        del inputs\n",
    "    if 'input_ids' in globals():\n",
    "        del input_ids\n",
    "\n",
    "    # Clear PyTorch cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Force Python's garbage collector to run\n",
    "    gc.collect()\n",
    "\n",
    "# Call the function to clear GPU memory\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ad3a6d4-18e5-4f94-86d9-a319f6444e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stream(transformers.StoppingCriteria):\n",
    "    def __init__(self, callback_func=None):\n",
    "        self.callback_func = callback_func\n",
    "\n",
    "    def __call__(self, input_ids, scores) -> bool:\n",
    "        if self.callback_func is not None:\n",
    "            self.callback_func(input_ids[0])\n",
    "        return False\n",
    "\n",
    "class Iteratorize:\n",
    "    \"\"\"\n",
    "    Transforms a function that takes a callback\n",
    "    into a lazy iterator (generator).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, func, kwargs={}, callback=None):\n",
    "        self.mfunc = func\n",
    "        self.c_callback = callback\n",
    "        self.q = Queue()\n",
    "        self.sentinel = object()\n",
    "        self.kwargs = kwargs\n",
    "        self.stop_now = False\n",
    "\n",
    "        def _callback(val):\n",
    "            if self.stop_now:\n",
    "                raise ValueError\n",
    "            self.q.put(val)\n",
    "\n",
    "        def gentask():\n",
    "            try:\n",
    "                ret = self.mfunc(callback=_callback, **self.kwargs)\n",
    "            except ValueError:\n",
    "                pass\n",
    "            except:\n",
    "                traceback.print_exc()\n",
    "                pass\n",
    "\n",
    "            self.q.put(self.sentinel)\n",
    "            if self.c_callback:\n",
    "                self.c_callback(ret)\n",
    "\n",
    "        self.thread = Thread(target=gentask)\n",
    "        self.thread.start()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        obj = self.q.get(True, None)\n",
    "        if obj is self.sentinel:\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.stop_now = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42929d04-a903-4ef5-9f8c-fe25fb1c655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompter(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
    "        self._verbose = verbose\n",
    "        template_name = \"alpaca\"\n",
    "        self.template = {\n",
    "            \"description\": \"Template used by Alpaca-LoRA.\",\n",
    "            \"prompt_input\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n",
    "            \"prompt_no_input\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n",
    "            \"response_split\": \"### Response:\"\n",
    "        }\n",
    "        if self._verbose:\n",
    "            print(\n",
    "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
    "            )\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "        # returns the full prompt from instruction and optional input\n",
    "        # if a label (=response, =output) is provided, it's also appended.\n",
    "        if input:\n",
    "            res = self.template[\"prompt_input\"].format(\n",
    "                instruction=instruction, input=input\n",
    "            )\n",
    "        else:\n",
    "            res = self.template[\"prompt_no_input\"].format(\n",
    "                instruction=instruction\n",
    "            )\n",
    "        if label:\n",
    "            res = f\"{res}{label}\"\n",
    "        if self._verbose:\n",
    "            print(res)\n",
    "        return res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.split(self.template[\"response_split\"])[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3cc17c5-0954-4156-8a43-c6f4957e1893",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06863e9c-c129-474c-a474-848737f71963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdbe0220-f33f-42e8-a9be-5fa2d7859a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'openthaigpt/openthaigpt-1.0.0-beta-7b-chat-ckpt-hf'\n",
    "lora_weights = None\n",
    "load_8bit = False\n",
    "prompt_template = \"\"\n",
    "server_name = \"0.0.0.0\"\n",
    "share_gradio = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ef5ef6a-d8d5-4ede-a399-bce5e341bb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "prompter = Prompter(prompt_template)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3f08786-f309-486f-bb07-ee48ac293bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:12<00:00,  6.36s/it]\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=load_8bit,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6815b964-1a72-4c13-ba8c-f43df1ffce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6170d4ef-da1b-470c-b170-195a4d1f191d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(56554, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=56554, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.half()  # seems to fix bugs for some users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f44e52e-26e1-4495-b715-b61c1e2d37df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu118\n",
      "win32\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__ )\n",
    "print(sys.platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b9576e9-4abb-4432-b9de-9b5685eba549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    instruction,\n",
    "    input=None,\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=128,\n",
    "    stream_output=False,\n",
    "    repetition_penalty=1,\n",
    "    no_repeat_ngram=0,\n",
    "    **kwargs,\n",
    "):\n",
    "    prompt = prompter.generate_prompt(instruction, input)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    generate_params = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"generation_config\": generation_config,\n",
    "        \"return_dict_in_generate\": True,\n",
    "        \"output_scores\": True,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"early_stopping\": True,\n",
    "        \"repetition_penalty\":repetition_penalty,\n",
    "        \"no_repeat_ngram_size\":no_repeat_ngram\n",
    "    }\n",
    "\n",
    "\n",
    "    if stream_output:\n",
    "        # Stream the reply 1 token at a time.\n",
    "        # This is based on the trick of using 'stopping_criteria' to create an iterator,\n",
    "        # from https://github.com/oobabooga/text-generation-webui/blob/ad37f396fc8bcbab90e11ecf17c56c97bfbd4a9c/modules/text_generation.py#L216-L243.\n",
    "\n",
    "        def generate_with_callback(callback=None, **kwargs):\n",
    "            kwargs.setdefault(\n",
    "                \"stopping_criteria\", transformers.StoppingCriteriaList()\n",
    "            )\n",
    "            kwargs[\"stopping_criteria\"].append(\n",
    "                Stream(callback_func=callback)\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                model.generate(**kwargs)\n",
    "\n",
    "        def generate_with_streaming(**kwargs):\n",
    "            return Iteratorize(\n",
    "                generate_with_callback, kwargs, callback=None\n",
    "            )\n",
    "\n",
    "        with generate_with_streaming(**generate_params) as generator:\n",
    "            for output in generator:\n",
    "                # new_tokens = len(output) - len(input_ids[0])\n",
    "                decoded_output = tokenizer.decode(output)\n",
    "\n",
    "                if output[-1] in [tokenizer.eos_token_id]:\n",
    "                    break\n",
    "\n",
    "                yield prompter.get_response(decoded_output)\n",
    "        return  # early return for stream_output\n",
    "\n",
    "    # Without streaming\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            early_stopping=True,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            no_repeat_ngram_size=no_repeat_ngram\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    yield prompter.get_response(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f47ef6b6-7a44-4f14-a93e-f95a4eccca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearText():\n",
    "    return [\"\",\"\",\"\"]\n",
    "\n",
    "def example1():\n",
    "    return [\"ลดความอ้วนต้องทำอย่างไร\",\"\"]\n",
    "\n",
    "def example2():\n",
    "    return [\"วางแผนเที่ยวในภูเก็ต แบบบริษัททัวร์\",\"ภูเก็ต เป็นจังหวัดหนึ่งทางภาคใต้ของประเทศไทย และเป็นเกาะขนาดใหญ่ที่สุดในประเทศไทย อยู่ในทะเลอันดามัน จังหวัดที่ใกล้เคียงทางทิศเหนือ คือ จังหวัดพังงา ทางทิศตะวันออก คือ จังหวัดพังงา ทั้งเกาะล้อมรอบด้วยมหาสมุทรอินเดีย และยังมีเกาะที่อยู่ในอาณาเขตของจังหวัดภูเก็ตทางทิศใต้และตะวันออก การเดินทางเข้าสู่ภูเก็ตนอกจากทางเรือแล้ว สามารถเดินทางโดยรถยนต์ซึ่งมีเพียงเส้นทางเดียวผ่านทางจังหวัดพังงา โดยข้ามสะพานสารสินและสะพานคู่ขนาน คือ สะพานท้าวเทพกระษัตรีและสะพานท้าวศรีสุนทร เพื่อเข้าสู่ตัวจังหวัด และทางอากาศโดยมีท่าอากาศยานนานาชาติภูเก็ตรองรับ ท่าอากาศยานนี้ตั้งอยู่ทางทิศตะวันตกเฉียงเหนือของเกาะ\"]\n",
    "\n",
    "def example3():\n",
    "    return [\"เขียนบทความเกี่ยวกับ \\\"ประโยชน์ของโกจิเบอร์รี่\\\"\",\"\"]\n",
    "\n",
    "def example4():\n",
    "    return [\"เขียนโค้ด\",\"python pandas csv export\"]\n",
    "\n",
    "def example5():\n",
    "    return [\"x+30=100 x=?\",\"\"]\n",
    "\n",
    "def example6():\n",
    "    return [\"แปลภาษาไทยเป็นอังกฤษ\",\"กรุงเทพมหานคร เป็นเมืองหลวงและนครที่มีประชากรมากที่สุดของประเทศไทย เป็นศูนย์กลางการปกครอง การศึกษา การคมนาคมขนส่ง การเงินการธนาคาร การพาณิชย์ การสื่อสาร และความเจริญของประเทศ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0051c46b-2eef-4232-a428-45fde6d80d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryu\\AppData\\Local\\Temp\\ipykernel_68160\\1102325564.py:32: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  outputbox = gr.inputs.Textbox(\n",
      "C:\\Users\\ryu\\AppData\\Local\\Temp\\ipykernel_68160\\1102325564.py:32: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  outputbox = gr.inputs.Textbox(\n",
      "C:\\Users\\ryu\\AppData\\Local\\Temp\\ipykernel_68160\\1102325564.py:32: GradioDeprecationWarning: `numeric` parameter is deprecated, and it has no effect\n",
      "  outputbox = gr.inputs.Textbox(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://d4cb9671a55e7dc3db.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d4cb9671a55e7dc3db.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # 🇹🇭 OpenThaiGPT 1.0.0-beta\n",
    "        🇹🇭 OpenThaiGPT Version 1.0.0-beta is a Thai language 7B-parameter LLaMA v2 Chat model finetuned to follow Thai translated instructions and extend 24,554 Thai words vocabularies for turbo speed. For more information, please visit [the project's website](https://openthaigpt.aieat.or.th/) | [Github](https://github.com/OpenThaiGPT/openthaigpt).\n",
    "\n",
    "        ## Examples\n",
    "        \"\"\"\n",
    "    )\n",
    "    with gr.Row():\n",
    "        example1_button = gr.Button(value=\"ลดความอ้วนต้องทำอย่างไร\")\n",
    "        example2_button = gr.Button(value=\"วางแผนเที่ยวในภูเก็ต แบบบริษัททัวร์\")\n",
    "        example3_button = gr.Button(value=\"เขียนบทความ\")\n",
    "        example4_button = gr.Button(value=\"เขียนโค้ด\")\n",
    "        example5_button = gr.Button(value=\"คำนวณคณิตศาสตร์\")\n",
    "        example6_button = gr.Button(value=\"แปลภาษา\")\n",
    "\n",
    "    instbox = gr.components.Textbox(\n",
    "            lines=2,\n",
    "            label=\"Instruction\",\n",
    "            placeholder=\"คำสั่ง\",\n",
    "            value=\"ลดความอ้วนต้องทำอย่างไร\"\n",
    "        )\n",
    "    inputbox = gr.components.Textbox(lines=2, label=\"Input\", placeholder=\"คำถาม (ไม่จำเป็น)\")\n",
    "    streambox = gr.components.Checkbox(label=\"Stream output\", value=True)\n",
    "    button = gr.Button(value=\"Generate\", variant=\"primary\")\n",
    "\n",
    "    with gr.Row():\n",
    "        cancel = gr.Button(value=\"Stop / Cancel\")\n",
    "        clear = gr.Button(value=\"Clear\")\n",
    "\n",
    "    outputbox = gr.inputs.Textbox(\n",
    "            lines=5,\n",
    "            label=\"Output\",\n",
    "        )\n",
    "\n",
    "    with gr.Accordion(\"Advanced Settings\", open=False):\n",
    "        tempbox = gr.components.Slider(\n",
    "            minimum=0, maximum=1, value=0.1, info=\"อุณหภูมิ: พารามิเตอร์นี้ใช้ควบคุมความเสี่ยงในการสร้างข้อความของระบบ ถ้าตั้งค่าไว้สูง การสร้างข้อความจะเป็นลักษณะที่หลากหลายมากขึ้น ถ้าตั้งค่าไว้ต่ำ การสร้างข้อความจะมีลักษณะที่มีโครงสร้างแน่นอนมากขึ้น\", label=\"Temperature\"\n",
    "        )\n",
    "        toppbox = gr.components.Slider(\n",
    "            minimum=0, maximum=1, value=0.75, info=\"nucleus sampling: พารามิเตอร์นี้ใช้เป็นวิธีการสุ่มตัวเลือกจากคำที่อาจจะถูกเลือกถัดไป ระบบจะสุ่มเลือกจากกลุ่มคำที่มีความน่าจะเป็นรวมกันสูงสุดถึง p%\", label=\"Top p\"\n",
    "        )\n",
    "        topkbox = gr.components.Slider(\n",
    "            minimum=0, maximum=100, step=1, value=40, info=\"top-k sampling: พารามิเตอร์นี้ใช้เลือก k คำที่มีความน่าจะเป็นสูงสุดสำหรับคำถัดไป แล้วจึงสุ่มเลือกหนึ่งใน k คำนั้น\", label=\"Top k\"\n",
    "        )\n",
    "        beambox = gr.components.Slider(\n",
    "            minimum=1, maximum=4, step=1, value=1, info=\"beam: จำนวนวิธีการสร้างข้อความโดยใช้คำหลายๆ ทางเลือกที่น่าจะเป็นที่สุดในแต่ละขั้นตอน การตั้งค่า Beam ที่สูงขึ้นจะทำให้สามารถสำรวจคำหลายทางเลือกมากขึ้น แต่จะเพิ่มการคำนวณและอาจจะไม่ทำให้ผลลัพธ์ดีขึ้นทุกครั้ง\", label=\"Beams\"\n",
    "        )\n",
    "        maxtokenbox = gr.components.Slider(\n",
    "            minimum=1, maximum=4096, step=1, value=512, info=\"max_token: ความยาวของคำตอบ\", label=\"Max tokens\"\n",
    "        )\n",
    "        repetition_penalty_box = gr.components.Slider(\n",
    "            minimum=1, maximum=1.99, step=0.01, value=1.2, info=\"repetition_penalty: ความรุนแรงในการลงโทษเมื่อตอบข้อความซ้ำ 1=ไม่ลงโทษ 1.99=ลงโทษสูงสุด\", label=\"Repetition Penalty\"\n",
    "        )\n",
    "        no_repeat_ngram_box = gr.components.Slider(\n",
    "            minimum=0, maximum=30, step=0, value=4, info=\"no_repeat_ngram: การป้องกันการตอบข้อความซ้ำตามจำนวนตัวอักษร\", label=\"No Repeat N-GRAM\"\n",
    "        )\n",
    "\n",
    "    button_click_event = button.click(fn=evaluate, inputs=[instbox, inputbox, tempbox, toppbox, topkbox, beambox, maxtokenbox, streambox, repetition_penalty_box, no_repeat_ngram_box], outputs=outputbox)\n",
    "    cancel.click(fn=None, inputs=None, outputs=None, cancels=[button_click_event])\n",
    "    clear.click(fn=clearText, outputs=[instbox, inputbox, outputbox])\n",
    "\n",
    "    example1_button.click(fn=example1, outputs=[instbox, inputbox])\n",
    "    example2_button.click(fn=example2, outputs=[instbox, inputbox])\n",
    "    example3_button.click(fn=example3, outputs=[instbox, inputbox])\n",
    "    example4_button.click(fn=example4, outputs=[instbox, inputbox])\n",
    "    example5_button.click(fn=example5, outputs=[instbox, inputbox])\n",
    "    example6_button.click(fn=example6, outputs=[instbox, inputbox])\n",
    "\n",
    "demo.queue().launch(server_name=\"0.0.0.0\", share=share_gradio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "998dddd3-e69f-41c8-8fd9-e4d0e520f05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_evaluate(\n",
    "      instruction,\n",
    "      input=None,\n",
    "      temperature=0.1,\n",
    "      top_p=0.75,\n",
    "      top_k=40,\n",
    "      num_beams=4,\n",
    "      max_new_tokens=512,\n",
    "      stream_output=False,\n",
    "      **kwargs,\n",
    "  ):\n",
    "      prompt = prompter.generate_prompt(instruction, input)\n",
    "      inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "      input_ids = inputs[\"input_ids\"].to(device)\n",
    "      generation_config = GenerationConfig(\n",
    "          temperature=temperature,\n",
    "          top_p=top_p,\n",
    "          top_k=top_k,\n",
    "          num_beams=num_beams,\n",
    "          **kwargs,\n",
    "      )\n",
    "\n",
    "      generate_params = {\n",
    "          \"input_ids\": input_ids,\n",
    "          \"generation_config\": generation_config,\n",
    "          \"return_dict_in_generate\": True,\n",
    "          \"output_scores\": True,\n",
    "          \"max_new_tokens\": max_new_tokens,\n",
    "      }\n",
    "\n",
    "      # Without streaming\n",
    "      with torch.no_grad():\n",
    "          generation_output = model.generate(\n",
    "              input_ids=input_ids,\n",
    "              generation_config=generation_config,\n",
    "              return_dict_in_generate=True,\n",
    "              output_scores=True,\n",
    "              max_new_tokens=max_new_tokens,\n",
    "          )\n",
    "      s = generation_output.sequences[0]\n",
    "      return tokenizer.decode(s).split(\"### Response:\")[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f7609ac-6ae9-4245-846a-3fe047ce9c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: ลดน้ำหนักต้องทำอย่างไร\n",
      "Response: การลดน้ำหนักสามารถทำได้หลายวิธี เช่น การออกกำลังกาเป็นประจำ การรับประทานอาหารที่มีประโยชน์ และการจำกัดปริมาณแคลอรี่ การออกกำลังกายสามารถทำได้หลายรูปแบบ เช่น วิ่ง ว่ายน้ำ ปั่นจักรยาน ว่ายน้ำ หรือยกน้ำหนัก การรับประทานอาหารที่มีประโยชน์สามารถทำได้โดยการบริโภคผัก ผลไม้ ธัญพืชไม่ขัดสี โปรตีนไม่ติดมัน และไขมันดีในปริมาณที่เพียงพอ การจำกัดปริมาณแคลอรี่สามารถทำได้โดยการจำกัดปริมาณแคลอรี่โดยรวมในแต่ละวันและลดปริมาณแคลอรี่ต่อหน่วยบริโภคในแต่ละมื้อ</s>\n",
      "\n",
      "Instruction: เขียนโปรแกรม python export csv pandas\n",
      "Response: โปรแกรม Python นี้ส่งออกข้อมูล Pandas เป็นไฟล์ CSV: import pandas as pd df = pandas.read_csv('file.csv') df.to_csv('file.csv')</s>\n",
      "\n",
      "Instruction: วิธีการทำน้ำจิ้มไก่\n",
      "Response: ในการทำน้ำจิ้มไก่ คุณจะต้องผสมน้ำส้มสายชู น้ำมะนาว น้ำตาลปี๊บ น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้ำมะนาว น้\n",
      "\n",
      "Instruction: แต่งกลอนวันแม่\n",
      "Response: เช้าวันแม่คือวันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ เช้าวันแม่คือวันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ เช้าวันแม่คือวันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ เช้าวันแม่คือวันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ เช้าวันแม่คือวันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ เช้าวันแม่คือวันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ เช้าวันแม่คือวันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ เช้าวันแม่คือวันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ เช้าวันแม่คือวันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ เช้าวันแม่คือวันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ เช้าวันแม่คือวันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ วันพิเศษสำหรับแม่และลูกๆ ของเธอ เช้าวันแม่คือวันพิเศษสำ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing with text_evaluate\n",
    "for instruction in [\n",
    "    \"ลดน้ำหนักต้องทำอย่างไร\",\n",
    "    \"เขียนโปรแกรม python export csv pandas\",\n",
    "    \"วิธีการทำน้ำจิ้มไก่\",\n",
    "    \"แต่งกลอนวันแม่\"\n",
    "]:\n",
    "    print(\"Instruction:\", instruction)\n",
    "    print(\"Response:\", text_evaluate(instruction))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8655bd5-d9cc-49f5-a02a-9a487f3eac31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "my-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5566a2e2-bced-4f41-9b3d-a4c01e662613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio==3.44.4 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (3.44.4)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (3.1.2)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (5.1.2)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (4.8.0)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (6.0.1)\n",
      "Requirement already satisfied: numpy~=1.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (1.26.1)\n",
      "Requirement already satisfied: matplotlib~=3.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (3.8.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (0.24.0.post1)\n",
      "Requirement already satisfied: httpx in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (0.0.6)\n",
      "Requirement already satisfied: gradio-client==0.5.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (0.5.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (2.1.3)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (6.1.1)\n",
      "Requirement already satisfied: markupsafe~=2.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (3.9.10)\n",
      "Requirement already satisfied: packaging in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (23.2)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (23.2.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (2.4.2)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (10.1.0)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (0.3.1)\n",
      "Requirement already satisfied: fastapi in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (0.104.1)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (2.10.0)\n",
      "Requirement already satisfied: pydub in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (0.25.1)\n",
      "Requirement already satisfied: requests~=2.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (2.31.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.14.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (0.17.3)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio==3.44.4) (11.0.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from gradio-client==0.5.1->gradio==3.44.4) (2023.10.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio==3.44.4) (4.19.2)\n",
      "Requirement already satisfied: toolz in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio==3.44.4) (0.12.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from huggingface-hub>=0.14.0->gradio==3.44.4) (4.66.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from huggingface-hub>=0.14.0->gradio==3.44.4) (3.13.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from importlib-resources<7.0,>=1.3->gradio==3.44.4) (3.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from matplotlib~=3.0->gradio==3.44.4) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from matplotlib~=3.0->gradio==3.44.4) (3.1.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from matplotlib~=3.0->gradio==3.44.4) (1.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from matplotlib~=3.0->gradio==3.44.4) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from matplotlib~=3.0->gradio==3.44.4) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from matplotlib~=3.0->gradio==3.44.4) (4.44.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from pandas<3.0,>=1.0->gradio==3.44.4) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from pandas<3.0,>=1.0->gradio==3.44.4) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.44.4) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.44.4) (2.10.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests~=2.0->gradio==3.44.4) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests~=2.0->gradio==3.44.4) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests~=2.0->gradio==3.44.4) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests~=2.0->gradio==3.44.4) (2023.7.22)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from uvicorn>=0.14.0->gradio==3.44.4) (0.14.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from uvicorn>=0.14.0->gradio==3.44.4) (8.1.7)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from fastapi->gradio==3.44.4) (0.27.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from fastapi->gradio==3.44.4) (3.7.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from httpx->gradio==3.44.4) (1.3.0)\n",
      "Requirement already satisfied: httpcore in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from httpx->gradio==3.44.4) (1.0.2)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio==3.44.4) (1.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from click>=7.0->uvicorn>=0.14.0->gradio==3.44.4) (0.4.6)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.44.4) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.44.4) (0.12.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.44.4) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.44.4) (2023.7.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==3.44.4) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\ryu\\Desktop\\llm\\openthaigpt-preview\\my-env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[sentencepiece] in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (4.35.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers[sentencepiece]) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers[sentencepiece]) (2.31.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers[sentencepiece]) (23.2)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers[sentencepiece]) (0.14.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers[sentencepiece]) (2023.10.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers[sentencepiece]) (3.13.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers[sentencepiece]) (0.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers[sentencepiece]) (0.17.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers[sentencepiece]) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers[sentencepiece]) (1.26.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers[sentencepiece]) (4.25.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers[sentencepiece]) (0.1.99)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers[sentencepiece]) (4.8.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers[sentencepiece]) (2023.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from tqdm>=4.27->transformers[sentencepiece]) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests->transformers[sentencepiece]) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests->transformers[sentencepiece]) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests->transformers[sentencepiece]) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests->transformers[sentencepiece]) (2.0.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\ryu\\Desktop\\llm\\openthaigpt-preview\\my-env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/peft.git\n",
      "  Cloning https://github.com/huggingface/peft.git to c:\\users\\ryu\\appdata\\local\\temp\\pip-req-build-75uahq3o\n",
      "  Resolved https://github.com/huggingface/peft.git to commit 5d84484079ee72c92678eadb273d3fe0241ed5ea\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from peft==0.6.2.dev0) (2.1.0+cu118)\n",
      "Requirement already satisfied: psutil in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from peft==0.6.2.dev0) (5.9.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from peft==0.6.2.dev0) (1.26.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from peft==0.6.2.dev0) (0.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from peft==0.6.2.dev0) (23.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from peft==0.6.2.dev0) (4.66.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from peft==0.6.2.dev0) (0.24.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from peft==0.6.2.dev0) (6.0.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from peft==0.6.2.dev0) (4.35.0)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from accelerate>=0.21.0->peft==0.6.2.dev0) (0.17.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torch>=1.13.0->peft==0.6.2.dev0) (2023.10.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torch>=1.13.0->peft==0.6.2.dev0) (3.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torch>=1.13.0->peft==0.6.2.dev0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torch>=1.13.0->peft==0.6.2.dev0) (4.8.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torch>=1.13.0->peft==0.6.2.dev0) (3.2.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torch>=1.13.0->peft==0.6.2.dev0) (1.12)\n",
      "Requirement already satisfied: colorama in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from tqdm->peft==0.6.2.dev0) (0.4.6)\n",
      "Requirement already satisfied: requests in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers->peft==0.6.2.dev0) (2.31.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers->peft==0.6.2.dev0) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from transformers->peft==0.6.2.dev0) (0.14.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from jinja2->torch>=1.13.0->peft==0.6.2.dev0) (2.1.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests->transformers->peft==0.6.2.dev0) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests->transformers->peft==0.6.2.dev0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests->transformers->peft==0.6.2.dev0) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests->transformers->peft==0.6.2.dev0) (3.3.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from sympy->torch>=1.13.0->peft==0.6.2.dev0) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git 'C:\\Users\\ryu\\AppData\\Local\\Temp\\pip-req-build-75uahq3o'\n",
      "WARNING: You are using pip version 22.0.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\ryu\\Desktop\\llm\\openthaigpt-preview\\my-env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio==3.44.4\n",
    "!pip install transformers[sentencepiece]\n",
    "!pip install git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3e9ba52-6f26-40d6-9b0e-1c5138263251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (0.16.0+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: filelock in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from torchvision) (1.26.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests->torchvision) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ryu\\desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\ryu\\Desktop\\llm\\openthaigpt-preview\\my-env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f8d0ba9-a44e-4909-ba4e-63984cf07964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryu\\Desktop\\llm\\openthaigpt-preview\\my-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import gradio as gr\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
    "import json\n",
    "import os.path as osp\n",
    "from typing import Union\n",
    "import gc\n",
    "import traceback\n",
    "from queue import Queue\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9032369-2421-471f-8d72-eac723bf68fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu_memory():\n",
    "    # Delete model and tensors if they are defined\n",
    "    global model, inputs, input_ids\n",
    "    if 'model' in globals():\n",
    "        del model\n",
    "    if 'inputs' in globals():\n",
    "        del inputs\n",
    "    if 'input_ids' in globals():\n",
    "        del input_ids\n",
    "\n",
    "    # Clear PyTorch cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Force Python's garbage collector to run\n",
    "    gc.collect()\n",
    "\n",
    "# Call the function to clear GPU memory\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ad3a6d4-18e5-4f94-86d9-a319f6444e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stream(transformers.StoppingCriteria):\n",
    "    def __init__(self, callback_func=None):\n",
    "        self.callback_func = callback_func\n",
    "\n",
    "    def __call__(self, input_ids, scores) -> bool:\n",
    "        if self.callback_func is not None:\n",
    "            self.callback_func(input_ids[0])\n",
    "        return False\n",
    "\n",
    "class Iteratorize:\n",
    "    \"\"\"\n",
    "    Transforms a function that takes a callback\n",
    "    into a lazy iterator (generator).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, func, kwargs={}, callback=None):\n",
    "        self.mfunc = func\n",
    "        self.c_callback = callback\n",
    "        self.q = Queue()\n",
    "        self.sentinel = object()\n",
    "        self.kwargs = kwargs\n",
    "        self.stop_now = False\n",
    "\n",
    "        def _callback(val):\n",
    "            if self.stop_now:\n",
    "                raise ValueError\n",
    "            self.q.put(val)\n",
    "\n",
    "        def gentask():\n",
    "            try:\n",
    "                ret = self.mfunc(callback=_callback, **self.kwargs)\n",
    "            except ValueError:\n",
    "                pass\n",
    "            except:\n",
    "                traceback.print_exc()\n",
    "                pass\n",
    "\n",
    "            self.q.put(self.sentinel)\n",
    "            if self.c_callback:\n",
    "                self.c_callback(ret)\n",
    "\n",
    "        self.thread = Thread(target=gentask)\n",
    "        self.thread.start()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        obj = self.q.get(True, None)\n",
    "        if obj is self.sentinel:\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.stop_now = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42929d04-a903-4ef5-9f8c-fe25fb1c655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompter(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
    "        self._verbose = verbose\n",
    "        template_name = \"alpaca\"\n",
    "        self.template = {\n",
    "            \"description\": \"Template used by Alpaca-LoRA.\",\n",
    "            \"prompt_input\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n",
    "            \"prompt_no_input\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n",
    "            \"response_split\": \"### Response:\"\n",
    "        }\n",
    "        if self._verbose:\n",
    "            print(\n",
    "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
    "            )\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "        # returns the full prompt from instruction and optional input\n",
    "        # if a label (=response, =output) is provided, it's also appended.\n",
    "        if input:\n",
    "            res = self.template[\"prompt_input\"].format(\n",
    "                instruction=instruction, input=input\n",
    "            )\n",
    "        else:\n",
    "            res = self.template[\"prompt_no_input\"].format(\n",
    "                instruction=instruction\n",
    "            )\n",
    "        if label:\n",
    "            res = f\"{res}{label}\"\n",
    "        if self._verbose:\n",
    "            print(res)\n",
    "        return res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.split(self.template[\"response_split\"])[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3cc17c5-0954-4156-8a43-c6f4957e1893",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06863e9c-c129-474c-a474-848737f71963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdbe0220-f33f-42e8-a9be-5fa2d7859a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'openthaigpt/openthaigpt-1.0.0-beta-7b-chat-ckpt-hf'\n",
    "lora_weights = None\n",
    "load_8bit = False\n",
    "prompt_template = \"\"\n",
    "server_name = \"0.0.0.0\"\n",
    "share_gradio = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ef5ef6a-d8d5-4ede-a399-bce5e341bb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "prompter = Prompter(prompt_template)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3f08786-f309-486f-bb07-ee48ac293bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.36s/it]\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=load_8bit,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6815b964-1a72-4c13-ba8c-f43df1ffce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6170d4ef-da1b-470c-b170-195a4d1f191d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(56554, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=56554, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.half()  # seems to fix bugs for some users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f44e52e-26e1-4495-b715-b61c1e2d37df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu118\n",
      "win32\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__ )\n",
    "print(sys.platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b9576e9-4abb-4432-b9de-9b5685eba549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    instruction,\n",
    "    input=None,\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=128,\n",
    "    stream_output=False,\n",
    "    repetition_penalty=1,\n",
    "    no_repeat_ngram=0,\n",
    "    **kwargs,\n",
    "):\n",
    "    prompt = prompter.generate_prompt(instruction, input)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    generate_params = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"generation_config\": generation_config,\n",
    "        \"return_dict_in_generate\": True,\n",
    "        \"output_scores\": True,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"early_stopping\": True,\n",
    "        \"repetition_penalty\":repetition_penalty,\n",
    "        \"no_repeat_ngram_size\":no_repeat_ngram\n",
    "    }\n",
    "\n",
    "\n",
    "    if stream_output:\n",
    "        # Stream the reply 1 token at a time.\n",
    "        # This is based on the trick of using 'stopping_criteria' to create an iterator,\n",
    "        # from https://github.com/oobabooga/text-generation-webui/blob/ad37f396fc8bcbab90e11ecf17c56c97bfbd4a9c/modules/text_generation.py#L216-L243.\n",
    "\n",
    "        def generate_with_callback(callback=None, **kwargs):\n",
    "            kwargs.setdefault(\n",
    "                \"stopping_criteria\", transformers.StoppingCriteriaList()\n",
    "            )\n",
    "            kwargs[\"stopping_criteria\"].append(\n",
    "                Stream(callback_func=callback)\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                model.generate(**kwargs)\n",
    "\n",
    "        def generate_with_streaming(**kwargs):\n",
    "            return Iteratorize(\n",
    "                generate_with_callback, kwargs, callback=None\n",
    "            )\n",
    "\n",
    "        with generate_with_streaming(**generate_params) as generator:\n",
    "            for output in generator:\n",
    "                # new_tokens = len(output) - len(input_ids[0])\n",
    "                decoded_output = tokenizer.decode(output)\n",
    "\n",
    "                if output[-1] in [tokenizer.eos_token_id]:\n",
    "                    break\n",
    "\n",
    "                yield prompter.get_response(decoded_output)\n",
    "        return  # early return for stream_output\n",
    "\n",
    "    # Without streaming\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            early_stopping=True,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            no_repeat_ngram_size=no_repeat_ngram\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    yield prompter.get_response(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f47ef6b6-7a44-4f14-a93e-f95a4eccca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearText():\n",
    "    return [\"\",\"\",\"\"]\n",
    "\n",
    "def example1():\n",
    "    return [\"‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡πâ‡∏ß‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£\",\"\"]\n",
    "\n",
    "def example2():\n",
    "    return [\"‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡πÉ‡∏ô‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï ‡πÅ‡∏ö‡∏ö‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏ó‡∏±‡∏ß‡∏£‡πå\",\"‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï ‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ó‡∏≤‡∏á‡∏†‡∏≤‡∏Ñ‡πÉ‡∏ï‡πâ‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢ ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Å‡∏≤‡∏∞‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢ ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ó‡∏∞‡πÄ‡∏•‡∏≠‡∏±‡∏ô‡∏î‡∏≤‡∏°‡∏±‡∏ô ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏ó‡∏≤‡∏á‡∏ó‡∏¥‡∏®‡πÄ‡∏´‡∏ô‡∏∑‡∏≠ ‡∏Ñ‡∏∑‡∏≠ ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏û‡∏±‡∏á‡∏á‡∏≤ ‡∏ó‡∏≤‡∏á‡∏ó‡∏¥‡∏®‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏≠‡∏≠‡∏Å ‡∏Ñ‡∏∑‡∏≠ ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏û‡∏±‡∏á‡∏á‡∏≤ ‡∏ó‡∏±‡πâ‡∏á‡πÄ‡∏Å‡∏≤‡∏∞‡∏•‡πâ‡∏≠‡∏°‡∏£‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏°‡∏´‡∏≤‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏≠‡∏¥‡∏ô‡πÄ‡∏î‡∏µ‡∏¢ ‡πÅ‡∏•‡∏∞‡∏¢‡∏±‡∏á‡∏°‡∏µ‡πÄ‡∏Å‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏≠‡∏≤‡∏ì‡∏≤‡πÄ‡∏Ç‡∏ï‡∏Ç‡∏≠‡∏á‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï‡∏ó‡∏≤‡∏á‡∏ó‡∏¥‡∏®‡πÉ‡∏ï‡πâ‡πÅ‡∏•‡∏∞‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏≠‡∏≠‡∏Å ‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ó‡∏≤‡∏á‡πÄ‡∏£‡∏∑‡∏≠‡πÅ‡∏•‡πâ‡∏ß ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡πÇ‡∏î‡∏¢‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ú‡πà‡∏≤‡∏ô‡∏ó‡∏≤‡∏á‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏û‡∏±‡∏á‡∏á‡∏≤ ‡πÇ‡∏î‡∏¢‡∏Ç‡πâ‡∏≤‡∏°‡∏™‡∏∞‡∏û‡∏≤‡∏ô‡∏™‡∏≤‡∏£‡∏™‡∏¥‡∏ô‡πÅ‡∏•‡∏∞‡∏™‡∏∞‡∏û‡∏≤‡∏ô‡∏Ñ‡∏π‡πà‡∏Ç‡∏ô‡∏≤‡∏ô ‡∏Ñ‡∏∑‡∏≠ ‡∏™‡∏∞‡∏û‡∏≤‡∏ô‡∏ó‡πâ‡∏≤‡∏ß‡πÄ‡∏ó‡∏û‡∏Å‡∏£‡∏∞‡∏©‡∏±‡∏ï‡∏£‡∏µ‡πÅ‡∏•‡∏∞‡∏™‡∏∞‡∏û‡∏≤‡∏ô‡∏ó‡πâ‡∏≤‡∏ß‡∏®‡∏£‡∏µ‡∏™‡∏∏‡∏ô‡∏ó‡∏£ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏ï‡∏±‡∏ß‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î ‡πÅ‡∏•‡∏∞‡∏ó‡∏≤‡∏á‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏ó‡πà‡∏≤‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏¢‡∏≤‡∏ô‡∏ô‡∏≤‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö ‡∏ó‡πà‡∏≤‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏¢‡∏≤‡∏ô‡∏ô‡∏µ‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏≤‡∏á‡∏ó‡∏¥‡∏®‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏ï‡∏Å‡πÄ‡∏â‡∏µ‡∏¢‡∏á‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡∏≤‡∏∞\"]\n",
    "\n",
    "def example3():\n",
    "    return [\"‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö \\\"‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏Ç‡∏≠‡∏á‡πÇ‡∏Å‡∏à‡∏¥‡πÄ‡∏ö‡∏≠‡∏£‡πå‡∏£‡∏µ‡πà\\\"\",\"\"]\n",
    "\n",
    "def example4():\n",
    "    return [\"‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î\",\"python pandas csv export\"]\n",
    "\n",
    "def example5():\n",
    "    return [\"x+30=100 x=?\",\"\"]\n",
    "\n",
    "def example6():\n",
    "    return [\"‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©\",\"‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£ ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏´‡∏•‡∏ß‡∏á‡πÅ‡∏•‡∏∞‡∏ô‡∏Ñ‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢ ‡πÄ‡∏õ‡πá‡∏ô‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏Å‡∏•‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏Å‡∏Ñ‡∏£‡∏≠‡∏á ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏°‡∏ô‡∏≤‡∏Ñ‡∏°‡∏Ç‡∏ô‡∏™‡πà‡∏á ‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£ ‡∏Å‡∏≤‡∏£‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå ‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£ ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏à‡∏£‡∏¥‡∏ç‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0051c46b-2eef-4232-a428-45fde6d80d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryu\\AppData\\Local\\Temp\\ipykernel_68160\\1102325564.py:32: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  outputbox = gr.inputs.Textbox(\n",
      "C:\\Users\\ryu\\AppData\\Local\\Temp\\ipykernel_68160\\1102325564.py:32: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  outputbox = gr.inputs.Textbox(\n",
      "C:\\Users\\ryu\\AppData\\Local\\Temp\\ipykernel_68160\\1102325564.py:32: GradioDeprecationWarning: `numeric` parameter is deprecated, and it has no effect\n",
      "  outputbox = gr.inputs.Textbox(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://d4cb9671a55e7dc3db.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d4cb9671a55e7dc3db.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # üáπüá≠ OpenThaiGPT 1.0.0-beta\n",
    "        üáπüá≠ OpenThaiGPT Version 1.0.0-beta is a Thai language 7B-parameter LLaMA v2 Chat model finetuned to follow Thai translated instructions and extend 24,554 Thai words vocabularies for turbo speed. For more information, please visit [the project's website](https://openthaigpt.aieat.or.th/) | [Github](https://github.com/OpenThaiGPT/openthaigpt).\n",
    "\n",
    "        ## Examples\n",
    "        \"\"\"\n",
    "    )\n",
    "    with gr.Row():\n",
    "        example1_button = gr.Button(value=\"‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡πâ‡∏ß‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£\")\n",
    "        example2_button = gr.Button(value=\"‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡πÉ‡∏ô‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï ‡πÅ‡∏ö‡∏ö‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏ó‡∏±‡∏ß‡∏£‡πå\")\n",
    "        example3_button = gr.Button(value=\"‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°\")\n",
    "        example4_button = gr.Button(value=\"‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î\")\n",
    "        example5_button = gr.Button(value=\"‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå\")\n",
    "        example6_button = gr.Button(value=\"‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤\")\n",
    "\n",
    "    instbox = gr.components.Textbox(\n",
    "            lines=2,\n",
    "            label=\"Instruction\",\n",
    "            placeholder=\"‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á\",\n",
    "            value=\"‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡πâ‡∏ß‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£\"\n",
    "        )\n",
    "    inputbox = gr.components.Textbox(lines=2, label=\"Input\", placeholder=\"‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)\")\n",
    "    streambox = gr.components.Checkbox(label=\"Stream output\", value=True)\n",
    "    button = gr.Button(value=\"Generate\", variant=\"primary\")\n",
    "\n",
    "    with gr.Row():\n",
    "        cancel = gr.Button(value=\"Stop / Cancel\")\n",
    "        clear = gr.Button(value=\"Clear\")\n",
    "\n",
    "    outputbox = gr.inputs.Textbox(\n",
    "            lines=5,\n",
    "            label=\"Output\",\n",
    "        )\n",
    "\n",
    "    with gr.Accordion(\"Advanced Settings\", open=False):\n",
    "        tempbox = gr.components.Slider(\n",
    "            minimum=0, maximum=1, value=0.1, info=\"‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥: ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö ‡∏ñ‡πâ‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏ß‡πâ‡∏™‡∏π‡∏á ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‡∏ñ‡πâ‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏ß‡πâ‡∏ï‡πà‡∏≥ ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏∞‡∏°‡∏µ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô\", label=\"Temperature\"\n",
    "        )\n",
    "        toppbox = gr.components.Slider(\n",
    "            minimum=0, maximum=1, value=0.75, info=\"nucleus sampling: ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏∏‡πà‡∏°‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ñ‡∏∂‡∏á p%\", label=\"Top p\"\n",
    "        )\n",
    "        topkbox = gr.components.Slider(\n",
    "            minimum=0, maximum=100, step=1, value=40, info=\"top-k sampling: ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å k ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∂‡∏á‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ô k ‡∏Ñ‡∏≥‡∏ô‡∏±‡πâ‡∏ô\", label=\"Top k\"\n",
    "        )\n",
    "        beambox = gr.components.Slider(\n",
    "            minimum=1, maximum=4, step=1, value=1, info=\"beam: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏´‡∏•‡∏≤‡∏¢‡πÜ ‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Beam ‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏≥‡∏£‡∏ß‡∏à‡∏Ñ‡∏≥‡∏´‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á\", label=\"Beams\"\n",
    "        )\n",
    "        maxtokenbox = gr.components.Slider(\n",
    "            minimum=1, maximum=4096, step=1, value=512, info=\"max_token: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö\", label=\"Max tokens\"\n",
    "        )\n",
    "        repetition_penalty_box = gr.components.Slider(\n",
    "            minimum=1, maximum=1.99, step=0.01, value=1.2, info=\"repetition_penalty: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏á‡πÇ‡∏ó‡∏©‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥ 1=‡πÑ‡∏°‡πà‡∏•‡∏á‡πÇ‡∏ó‡∏© 1.99=‡∏•‡∏á‡πÇ‡∏ó‡∏©‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î\", label=\"Repetition Penalty\"\n",
    "        )\n",
    "        no_repeat_ngram_box = gr.components.Slider(\n",
    "            minimum=0, maximum=30, step=0, value=4, info=\"no_repeat_ngram: ‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£\", label=\"No Repeat N-GRAM\"\n",
    "        )\n",
    "\n",
    "    button_click_event = button.click(fn=evaluate, inputs=[instbox, inputbox, tempbox, toppbox, topkbox, beambox, maxtokenbox, streambox, repetition_penalty_box, no_repeat_ngram_box], outputs=outputbox)\n",
    "    cancel.click(fn=None, inputs=None, outputs=None, cancels=[button_click_event])\n",
    "    clear.click(fn=clearText, outputs=[instbox, inputbox, outputbox])\n",
    "\n",
    "    example1_button.click(fn=example1, outputs=[instbox, inputbox])\n",
    "    example2_button.click(fn=example2, outputs=[instbox, inputbox])\n",
    "    example3_button.click(fn=example3, outputs=[instbox, inputbox])\n",
    "    example4_button.click(fn=example4, outputs=[instbox, inputbox])\n",
    "    example5_button.click(fn=example5, outputs=[instbox, inputbox])\n",
    "    example6_button.click(fn=example6, outputs=[instbox, inputbox])\n",
    "\n",
    "demo.queue().launch(server_name=\"0.0.0.0\", share=share_gradio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "998dddd3-e69f-41c8-8fd9-e4d0e520f05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_evaluate(\n",
    "      instruction,\n",
    "      input=None,\n",
    "      temperature=0.1,\n",
    "      top_p=0.75,\n",
    "      top_k=40,\n",
    "      num_beams=4,\n",
    "      max_new_tokens=512,\n",
    "      stream_output=False,\n",
    "      **kwargs,\n",
    "  ):\n",
    "      prompt = prompter.generate_prompt(instruction, input)\n",
    "      inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "      input_ids = inputs[\"input_ids\"].to(device)\n",
    "      generation_config = GenerationConfig(\n",
    "          temperature=temperature,\n",
    "          top_p=top_p,\n",
    "          top_k=top_k,\n",
    "          num_beams=num_beams,\n",
    "          **kwargs,\n",
    "      )\n",
    "\n",
    "      generate_params = {\n",
    "          \"input_ids\": input_ids,\n",
    "          \"generation_config\": generation_config,\n",
    "          \"return_dict_in_generate\": True,\n",
    "          \"output_scores\": True,\n",
    "          \"max_new_tokens\": max_new_tokens,\n",
    "      }\n",
    "\n",
    "      # Without streaming\n",
    "      with torch.no_grad():\n",
    "          generation_output = model.generate(\n",
    "              input_ids=input_ids,\n",
    "              generation_config=generation_config,\n",
    "              return_dict_in_generate=True,\n",
    "              output_scores=True,\n",
    "              max_new_tokens=max_new_tokens,\n",
    "          )\n",
    "      s = generation_output.sequences[0]\n",
    "      return tokenizer.decode(s).split(\"### Response:\")[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f7609ac-6ae9-4245-846a-3fe047ce9c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: ‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£\n",
      "Response: ‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡∏à‡∏≥ ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡πÅ‡∏Ñ‡∏•‡∏≠‡∏£‡∏µ‡πà ‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏¢‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö ‡πÄ‡∏ä‡πà‡∏ô ‡∏ß‡∏¥‡πà‡∏á ‡∏ß‡πà‡∏≤‡∏¢‡∏ô‡πâ‡∏≥ ‡∏õ‡∏±‡πà‡∏ô‡∏à‡∏±‡∏Å‡∏£‡∏¢‡∏≤‡∏ô ‡∏ß‡πà‡∏≤‡∏¢‡∏ô‡πâ‡∏≥ ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏Å‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡πÇ‡∏†‡∏Ñ‡∏ú‡∏±‡∏Å ‡∏ú‡∏•‡πÑ‡∏°‡πâ ‡∏ò‡∏±‡∏ç‡∏û‡∏∑‡∏ä‡πÑ‡∏°‡πà‡∏Ç‡∏±‡∏î‡∏™‡∏µ ‡πÇ‡∏õ‡∏£‡∏ï‡∏µ‡∏ô‡πÑ‡∏°‡πà‡∏ï‡∏¥‡∏î‡∏°‡∏±‡∏ô ‡πÅ‡∏•‡∏∞‡πÑ‡∏Ç‡∏°‡∏±‡∏ô‡∏î‡∏µ‡πÉ‡∏ô‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡πÅ‡∏Ñ‡∏•‡∏≠‡∏£‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡πÅ‡∏Ñ‡∏•‡∏≠‡∏£‡∏µ‡πà‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏•‡∏î‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡πÅ‡∏Ñ‡∏•‡∏≠‡∏£‡∏µ‡πà‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏ö‡∏£‡∏¥‡πÇ‡∏†‡∏Ñ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏°‡∏∑‡πâ‡∏≠</s>\n",
      "\n",
      "Instruction: ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° python export csv pandas\n",
      "Response: ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° Python ‡∏ô‡∏µ‡πâ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Pandas ‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV: import pandas as pd df = pandas.read_csv('file.csv') df.to_csv('file.csv')</s>\n",
      "\n",
      "Instruction: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡πâ‡∏≥‡∏à‡∏¥‡πâ‡∏°‡πÑ‡∏Å‡πà\n",
      "Response: ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡πâ‡∏≥‡∏à‡∏¥‡πâ‡∏°‡πÑ‡∏Å‡πà ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏ú‡∏™‡∏°‡∏ô‡πâ‡∏≥‡∏™‡πâ‡∏°‡∏™‡∏≤‡∏¢‡∏ä‡∏π ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏ï‡∏≤‡∏•‡∏õ‡∏µ‡πä‡∏ö ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ‡∏≥‡∏°‡∏∞‡∏ô‡∏≤‡∏ß ‡∏ô‡πâ\n",
      "\n",
      "Instruction: ‡πÅ‡∏ï‡πà‡∏á‡∏Å‡∏•‡∏≠‡∏ô‡∏ß‡∏±‡∏ô‡πÅ‡∏°‡πà\n",
      "Response: ‡πÄ‡∏ä‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡πÅ‡∏°‡πà‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡πÄ‡∏ä‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡πÅ‡∏°‡πà‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡πÄ‡∏ä‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡πÅ‡∏°‡πà‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡πÄ‡∏ä‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡πÅ‡∏°‡πà‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡πÄ‡∏ä‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡πÅ‡∏°‡πà‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡πÄ‡∏ä‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡πÅ‡∏°‡πà‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡πÄ‡∏ä‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡πÅ‡∏°‡πà‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡πÄ‡∏ä‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡πÅ‡∏°‡πà‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡πÄ‡∏ä‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡πÅ‡∏°‡πà‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡πÄ‡∏ä‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡πÅ‡∏°‡πà‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡πÄ‡∏ä‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡πÅ‡∏°‡πà‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ò‡∏≠ ‡πÄ‡∏ä‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡πÅ‡∏°‡πà‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing with text_evaluate\n",
    "for instruction in [\n",
    "    \"‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£\",\n",
    "    \"‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° python export csv pandas\",\n",
    "    \"‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡πâ‡∏≥‡∏à‡∏¥‡πâ‡∏°‡πÑ‡∏Å‡πà\",\n",
    "    \"‡πÅ‡∏ï‡πà‡∏á‡∏Å‡∏•‡∏≠‡∏ô‡∏ß‡∏±‡∏ô‡πÅ‡∏°‡πà\"\n",
    "]:\n",
    "    print(\"Instruction:\", instruction)\n",
    "    print(\"Response:\", text_evaluate(instruction))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8655bd5-d9cc-49f5-a02a-9a487f3eac31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "my-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
